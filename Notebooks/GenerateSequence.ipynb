{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchtext.vocab import vocab\n",
    "from collections import OrderedDict\n",
    "import random\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00b84d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition des tokens utilisés par notre transformer\n",
    "\n",
    "\n",
    "NOTE_SIZE = 128\n",
    "DUR_SIZE = 160\n",
    "TIM_SIZE = 1000\n",
    "VEL_SIZE = 128\n",
    "\n",
    "\n",
    "NOTE_TOKS = [f'n{i}' for i in range(NOTE_SIZE)] \n",
    "DUR_TOKS = [f'd{i}' for i in range(DUR_SIZE)]\n",
    "TIM_TOKS = [f't{i}' for i in range(TIM_SIZE)]\n",
    "VEL_TOKS = [f'v{i}' for i in range(VEL_SIZE)]\n",
    "\n",
    "BOS_TOK = \"BOS\"\n",
    "# Le token dummy sert seulement à initialiser les mots du vocab à partir de l'index 1, conformément aux prérequis de la fonction vocab()\n",
    "VOCAB = [\"dummy\"] + [BOS_TOK] + NOTE_TOKS + DUR_TOKS + TIM_TOKS + VEL_TOKS \n",
    "\n",
    "DICT = [(element, index) for index, element in enumerate(VOCAB)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb1e4d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition du vocabulaire\n",
    "custom_vocab = vocab(OrderedDict(DICT))\n",
    "itos_vocab = custom_vocab.get_itos()\n",
    "\n",
    "vocab_size = len(custom_vocab)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les deux cellules suivantes permettent de définir l'architecture utilisée (pour que le fichier du modèle puisse y définir les poids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, dim_model, dropout_p, max_len):\n",
    "        super().__init__()\n",
    "        # Modified version from: https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "        # max_len determines how far the position can have an effect on a token (window)\n",
    "        \n",
    "        # Info\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        \n",
    "        # Encoding - From formula\n",
    "        pos_encoding = torch.zeros(max_len, dim_model)\n",
    "        positions_list = torch.arange(0, max_len, dtype=torch.float).view(-1, 1) # 0, 1, 2, 3, 4, 5\n",
    "        division_term = torch.exp(torch.arange(0, dim_model, 2).float() * (-math.log(10000.0)) / dim_model) # 1000^(2i/dim_model)\n",
    "        \n",
    "        # PE(pos, 2i) = sin(pos/1000^(2i/dim_model))\n",
    "        pos_encoding[:, 0::2] = torch.sin(positions_list * division_term)\n",
    "        \n",
    "        # PE(pos, 2i + 1) = cos(pos/1000^(2i/dim_model))\n",
    "        pos_encoding[:, 1::2] = torch.cos(positions_list * division_term)\n",
    "        \n",
    "        # Saving buffer (same as parameter without gradients needed)\n",
    "        pos_encoding = pos_encoding.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer(\"pos_encoding\",pos_encoding)\n",
    "        \n",
    "    def forward(self, token_embedding: torch.tensor) -> torch.tensor:\n",
    "        # Residual connection + pos encoding\n",
    "        return self.dropout(token_embedding + self.pos_encoding[:token_embedding.size(0), :])\n",
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Model from \"A detailed guide to Pytorch's nn.Transformer() module.\", by\n",
    "    Daniel Melchor: https://medium.com/p/c80afbc9ffb1/\n",
    "    \"\"\"\n",
    "    # Constructor\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_tokens,\n",
    "        dim_model,\n",
    "        num_heads,\n",
    "        num_encoder_layers,\n",
    "        num_decoder_layers,\n",
    "        dropout_p,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # INFO\n",
    "        self.model_type = \"Transformer\"\n",
    "        self.dim_model = dim_model\n",
    "\n",
    "        # LAYERS\n",
    "        self.positional_encoder = PositionalEncoding(\n",
    "            dim_model=dim_model, dropout_p=dropout_p, max_len=5000\n",
    "        )\n",
    "        self.embedding = nn.Embedding(num_tokens, dim_model)\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=dim_model,\n",
    "            nhead=num_heads,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dropout=dropout_p,\n",
    "            batch_first = True\n",
    "        )\n",
    "        self.out = nn.Linear(dim_model, num_tokens)\n",
    "    # A modifier pour utiliser 4 out functions différentes selon les cas    \n",
    "    def forward(self, src, tgt, tgt_mask=None, src_pad_mask=None, tgt_pad_mask=None):\n",
    "        # Src size must be (batch_size, src sequence length)\n",
    "        # Tgt size must be (batch_size, tgt sequence length)\n",
    "\n",
    "        # Embedding + positional encoding - Out size = (batch_size, sequence length, dim_model)\n",
    "        src = self.embedding(src) * math.sqrt(self.dim_model)\n",
    "        tgt = self.embedding(tgt) * math.sqrt(self.dim_model)\n",
    "        src = self.positional_encoder(src)\n",
    "        tgt = self.positional_encoder(tgt)\n",
    "        \n",
    "        transformer_out = self.transformer(src, tgt, tgt_mask=tgt_mask, src_key_padding_mask=src_pad_mask, tgt_key_padding_mask=tgt_pad_mask)\n",
    "        out = self.out(transformer_out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "    # Genere un masque triangulaire  \n",
    "    def get_tgt_mask(self, size) -> torch.tensor:\n",
    "        # Generates a squeare matrix where the each row allows one word more to be seen\n",
    "        mask = torch.tril(torch.ones(size, size) == 1) # Lower triangular matrix\n",
    "        mask = mask.float()\n",
    "        mask = mask.masked_fill(mask == 0, float('-inf')) # Convert zeros to -inf\n",
    "        mask = mask.masked_fill(mask == 1, float(0.0)) # Convert ones to 0\n",
    "        \n",
    "        # EX for size=5:\n",
    "        # [[0., -inf, -inf, -inf, -inf],\n",
    "        #  [0.,   0., -inf, -inf, -inf],\n",
    "        #  [0.,   0.,   0., -inf, -inf],\n",
    "        #  [0.,   0.,   0.,   0., -inf],\n",
    "        #  [0.,   0.,   0.,   0.,   0.]]\n",
    "        \n",
    "        return mask\n",
    "    \n",
    "    # Le pad mask sera utile quand on aura ajouté les PAD tokens\n",
    "    # def create_pad_mask(self, matrix: torch.tensor, pad_token: int) -> torch.tensor:\n",
    "    #     # If matrix = [1,2,3,0,0,0] where pad_token=0, the result mask is\n",
    "    #     # [False, False, False, True, True, True]\n",
    "    #     return (matrix == pad_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (positional_encoder): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (embedding): Embedding(1417, 512)\n",
       "  (transformer): Transformer(\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (3): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (out): Linear(in_features=512, out_features=1417, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# On charge le modèle pré entraîné\n",
    "model = Transformer(\n",
    "num_tokens=len(custom_vocab), dim_model=512, num_heads=8, num_encoder_layers=1, num_decoder_layers=4, dropout_p=0.1)\n",
    "model.load_state_dict(torch.load(\"modelperf.pth\",map_location=torch.device(device)))\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Génération"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L'input donné à l'encoder (vecteur nul dans notre cas, comme pendant l'entraînement)\n",
    "taille_bloc = 120\n",
    "X = torch.tensor([0]*taille_bloc).unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def generate_sequence(model, start_tokens, max_length=100, temperature=1.0):\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tokens = start_tokens\n",
    "        for _ in range(max_length - len(start_tokens)):\n",
    "            # Unsqueeze(0) rajoute une dimension qui correspond au batch_size (qui vaut 1 dans ce cas) pour coller aux shape attendues par le modèle\n",
    "            input_tokens = torch.tensor(tokens).unsqueeze(0).to(device)\n",
    "            \n",
    "            output = model(X, input_tokens)\n",
    "            # Les logits sont des probabilits non normalisées. La température contrôle leur dispersion : permet d'ajouter plus ou moins de bruit lors de la prédiction\n",
    "            logits = output[:, -1, :] / temperature\n",
    "            # Softmax transforme les logits en probabilités, multinomial fait une séleciton pondérée par ces probabilités d''un seul indice (num_samples=1), to_list passe de tensor à array\n",
    "            next_token = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1).squeeze().tolist()\n",
    "            tokens.append(next_token)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated sequence: ['n65', 'd2', 'v104', 'n63', 'd2', 't4', 'v109', 'n58', 'd1', 't3', 'v103', 'n61', 'd3', 't2', 'v104', 'n63', 'd2', 't4', 'v114', 'n58', 'd2', 't5', 'v106', 'n63', 'd2', 't2', 'v118', 'n63', 'd6', 't3', 'v114', 'n77', 'd2', 't16', 'v100', 'n57', 'd1', 't2', 'v100', 'n61', 'd1', 't2', 'v96', 'n65', 'd1', 't3', 'v106', 'n62', 'd1', 't2', 'v118', 'n64', 'd1', 't2', 'v105', 'n62', 'd1', 't0', 'v111', 'n77', 'd2', 't1', 'v118', 'n69', 'd2', 't2', 'v119', 'n63', 'd2', 't2', 'v113', 'n80', 'd1', 't2', 'v106', 'n58', 'd2', 't2', 'v111', 'n24', 'v38', 'v109', 'n65', 'd1', 't1', 'v101', 'n62', 'd1', 't2', 'v93', 'n63', 'd1', 't2', 'v112', 'n67', 'd1', 't532', 'n72', 'd2', 't4', 'v115', 'n63', 'd1', 't0', 'v110', 'n60', 'd1', 't2', 'v107', 'n58', 'd2', 't1', 'v117', 'n62', 'd1', 't5', 'v93', 'n76', 'd1', 't2', 'v102', 'n60', 'd1', 't2', 'v112', 'n68', 'd2', 't15', 'v101', 'n62', 'd1', 't25', 'v101', 'n74', 'd1', 't2', 'v104', 'n67', 'd1', 't1', 'v117', 'n56', 'd2', 't2', 'v107', 'n64', 'd1', 't5', 't15', 'd9', 't4', 'v119', 'n68', 'd1', 't2', 'v103', 'n62', 'd1', 't1', 'v100', 'n61', 'd1', 't13', 'v117', 'n55', 'd1', 't2', 'v114', 'n63', 'd1', 't3', 'v118', 'n74', 'd2', 't2', 'v112', 'n70', 'd2', 't13', 'v115', 'n63', 'd2', 't2', 'v114', 'n67', 'd2', 't5', 'v114', 'n74', 'd2', 't1', 'v116', 'n75', 'd2', 't1', 'v109', 'n64', 'd1', 't4', 'v106', 'n70', 'd1', 't2', 'v109', 'n81', 'd2', 't2', 'v86', 'n61', 'd1', 't2', 'v113', 'n58', 'd2', 't0', 'v107', 'n71', 'd2', 't5', 't569', 'v119', 'n64', 'd1', 't2', 'v119', 'n61', 'd2', 't1', 'v118', 'n76', 'd1', 't1', 'v118', 'n53', 'd1', 't12', 'v117', 'n74', 'd2', 't5', 'v118', 'n63', 'd1', 't1', 'v107', 'n65', 'd1', 't2', 'v108', 'n67', 'd2', 't2', 'v105', 'n55', 'd2', 't2', 'v106', 'n64', 'd2', 't1', 'v113', 't293', 'n66', 'd2', 't13', 'v118', 'n68', 'd2', 't2', 'v106', 'n58', 'd1', 't1', 'v114', 'n71', 'd1', 't2', 'v111', 'n58', 'd2', 't2', 'v112', 'n56', 'd1', 't2', 'v108', 'n70', 'd2', 't1', 'v111', 'n77', 'd2', 't0', 'v117', 'n74', 'd2', 't1', 'v119', 'n68', 'd1', 't2', 'v114', 'n66', 'd2', 't2', 'v116', 'n72', 'd1', 't2', 'v113', 'n66', 'd2', 't825', 'v106', 'n63', 'd2', 't12', 'v109', 'n64', 'd2', 't2', 'v117', 'n77', 'd2', 't2', 'v113', 'n58', 'd1', 't2', 'v114', 'n58', 'd1', 't1', 'v102', 'n63', 'd2', 't12', 'v115', 'n63', 'd2', 't2', 'v121', 't378', 'v98', 'n62', 'd1', 't16', 'v100', 'n65', 'd1', 't2', 'v113', 'n78', 'd1', 't3', 'v109', 'n61', 'd1', 't1', 'v111', 'n77', 'd2', 't1', 'v114', 'n75', 'd3', 't3', 'v95', 'n61', 'd1', 't24', 'v101', 'n60', 'd1', 't2', 'v122', 'n72', 'd1', 't2', 'v119', 'n75', 'd6', 't4', 'v110', 'n64', 'd6', 't3', 'v106', 'n77', 'd2', 't2', 'v107', 'n63', 'd1', 't2', 'v120', 'n69', 'd1', 't1', 'v117', 'n67', 'd2', 't3', 'v109', 'n75', 'd1', 't6', 'v115', 'n70', 'd2', 't1', 'v121', 'n60', 'd1', 't4', 'v123', 'n69', 'd2', 't2', 'v105', 'n53', 'd2', 't2', 'v113', 'n82', 'd2', 't2', 'v111', 'n54', 'd1', 't29', 'v115', 'n70', 'd2', 't1', 'v100', 'n59', 'd1', 't2', 'v104', 'n67', 'd1', 't16', 'v111', 'n67', 'd2', 't3', 'v101', 'n62', 'd1', 't1', 'v109', 'n56', 'd1', 't1', 'v111', 'n62', 'd2', 't1', 'd55', 'd2', 't1', 'v107', 'n64', 'd1', 't2', 'v108', 'n74', 'd2', 't6', 'n76', 'd2', 't2', 'v118', 'n73', 'd2', 't2', 'v107', 'n72', 'd2', 't119', 'n77', 'd2', 't12', 't19', 'n8', 't185', 'v95', 'n63', 'd2', 't2', 'v105', 'n70', 'd1', 't2', 'v122', 'n58', 'd2', 't2', 'v116']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the start tokens for your inference\n",
    "#start_tokens = [custom_vocab[\"n60\"], custom_vocab[\"d1\"], custom_vocab[\"t1\"], custom_vocab[\"v64\"]]\n",
    "start_tokens = ['n65','d2','v104','n63','d2','t4','v109','n58','d1','t3','v103','n61','d3','t2','v104','n63','d2','t4','v114','n58','d2','t5','v106']\n",
    "start_tokens = [custom_vocab[el] for el in start_tokens]\n",
    "# Generate a sequence of tokens\n",
    "generated_tokens = generate_sequence(model, start_tokens, max_length=500, temperature=1.0)\n",
    "\n",
    "# Decode the generated tokens into the original format\n",
    "decoded_tokens = [itos_vocab[el] for el in generated_tokens]\n",
    "np.save(\"generation4.npy\", decoded_tokens)\n",
    "\n",
    "print(\"Generated sequence:\", decoded_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Inference",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "62c874bf93cba6215fcbca84adbfbe32b2ca0e29f0c7e59c6c1a61786fb9dcf2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
